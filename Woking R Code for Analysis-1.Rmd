---
title: "Working R code for Analysis"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    collapsed: false
    number_sections: true
---


# Set up
```{r}
# All of libraries 
library(knitr)
library(dplyr)
library(ggplot2)
library(cluster)
library(factoextra)
library(psych)
library(tidyverse)
library(lubridate)
library(gridExtra)
library(broom)
library(ggmap)
library(ggrepel)
library(forcats)
library(dbscan)
library(RColorBrewer)
library(kableExtra)
library(geosphere)
library(TSP)

# Google Key
register_google(key = "AIzaSyD2H7Xy0yI1hEONVTzQciE-D5cRFSlOIZw")

# knitr options
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Principal Component Analysis (PCA) (Correct Version)
## Data Preparation
```{r}
# Read and subset data
amazondata <- read.csv("/Users/Louie/Desktop/cleaned_dataset.csv")

selected_vars <- c("PackageVolume_cm3", "ExecutorCapacity", "LoadUtilization", 
                   "PlannedServiceTime", "DeliveryDuration_HR", "DistanceToNextStop")

# Remove missing values and scale the selected features
amazon_pca_data <- amazondata %>%
  select(all_of(selected_vars)) %>%
  na.omit() %>%
  scale()

# Subsample 8000 rows for computational efficiency
set.seed(123)
amazon_subset <- as.data.frame(amazon_pca_data) %>% sample_n(8000)
```

## Suitability Check for PCA
```{r}
# Correlation matrix
cor_matrix <- cor(amazon_subset)

# KMO Test
kmo_result <- KMO(cor_matrix)
kmo_result

# Bartlett's Test
bartlett_test <- cortest.bartlett(cor_matrix, n = nrow(amazon_subset))
bartlett_test
```

## Run PCA
```{r}
# Run PCA
pca_result <- prcomp(amazon_subset, center = TRUE, scale. = TRUE)

# Variance explained
summary(pca_result)

# Scree plot
fviz_eig(pca_result, addlabels = TRUE, ylim = c(0, 50))
```

## View Loadings For Top 5 Principal Components
```{r}
# Loadings for top 5 PCs
round(pca_result$rotation[, 1:5], 3)
```

## Visualize PCA Individuals
```{r}
# PCA individuals plot
fviz_pca_ind(pca_result,
             geom.ind = "point",
             col.ind = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE)
```

## Determine Optimal K for Clustering
```{r}
# Extract first three principal components
pca_df <- as.data.frame(pca_result$x[, 1:3])

# Calculate WSS and Ratio
within_ss = sapply(1:9, FUN = function(x) kmeans(pca_df, centers = x, iter.max = 100)$tot.withinss)
ratio_ss = sapply(1:9, FUN = function(x) {
  km = kmeans(pca_df, centers = x, iter.max = 100)
  ratio = km$betweenss/km$totss
  return(ratio)
})

# Combine into a dataframe
dat = data.frame(clusters = 1:9, within_ss, ratio_ss)

# Ratio Plot
ggplot(dat, aes(x = clusters, y = ratio_ss)) +
  geom_line(color = 'steelblue', linewidth = 1.4) +
  scale_x_continuous(breaks = 1:9) +
  geom_vline(xintercept = 3, linetype = 2) +
  labs(title = "Ratio of Between to Total SS", x = "Number of Clusters", y = "Ratio")
```

## K-Means Clustering with K = 3
```{r}
# Run K-Means
set.seed(123)
kmeans_result <- kmeans(pca_df, centers = 3, nstart = 25)
pca_df$cluster <- factor(kmeans_result$cluster)

# Clustered PCA scatter plot
ggplot(pca_df, aes(x = PC1, y = PC2, color = cluster)) +
  geom_point(size = 2, alpha = 0.7) +
  labs(title = "K-Means Clustering based on PCA", x = "PC1", y = "PC2") +
  theme_minimal()
```

## Interpretation of Clusters
```{r}
# Interpretation of clusters
pca_scores <- as.data.frame(pca_result$x[, 1:3])
pca_scores$Cluster <- as.factor(kmeans_result$cluster)

aggregate(. ~ Cluster, data = pca_scores, mean)
```


# Load Utilization (Wrong Version)
## Select relevant fields
```{r}
load_data <- amazondata %>%
  select(RouteID, StationCode, PackageID, LoadUtilization, DeliveryDuration_HR, PlannedServiceTime) %>%
  drop_na()
```

## Convert PlannedServiceTime to numeric if not already
```{r}
load_data <- load_data %>%
  mutate(PlannedServiceTime = as.numeric(PlannedServiceTime))
```

## Calculate Packages Delivered per Stop
```{r}
packages_per_stop <- load_data %>%
  group_by(RouteID, StationCode, PlannedServiceTime) %>%
  summarise(PackagesAtStop = n(), .groups = 'drop')
```

## Arrange by Route and PlannedServiceTime
```{r}
packages_ordered <- packages_per_stop %>%
  arrange(RouteID, PlannedServiceTime) %>%
  group_by(RouteID) %>%
  mutate(RemainingPackages = rev(cumsum(rev(PackagesAtStop))),
         StopOrder = row_number()) %>%
  ungroup()

head(packages_ordered)
```

## Plot Packages Delivered per Stop (these versions are not correct)
```{r}
# Ver 1
p1 <- ggplot(packages_ordered, aes(x = PackagesAtStop)) +
  geom_histogram(binwidth = 1, fill = 'steelblue', color = 'white', alpha = 0.7) +
  theme_minimal() +
  labs(title = "Distribution of Packages Delivered at Each Stop",
       x = "Packages Delivered", y = "Count")
p1

p2 <- ggplot(packages_ordered, aes(x = RemainingPackages)) +
  geom_histogram(binwidth = 5, fill = 'darkgreen', color = 'white', alpha = 0.7) +
  theme_minimal() +
  labs(title = "Fixed: Remaining Packages Distribution",
       x = "Remaining Packages", y = "Count")
p2

load_data_full <- load_data %>%
  left_join(packages_ordered %>% select(RouteID, StationCode, PackagesAtStop, RemainingPackages),
            by = c("RouteID", "StationCode"))

reg_model2 <- lm(DeliveryDuration_HR ~ LoadUtilization + PackagesAtStop, data = load_data_full)

# Ver 2
p1 <- ggplot(load_data, aes(x = PackagesAtStop)) +
  geom_histogram(binwidth = 1, fill = 'steelblue', color = 'white', alpha = 0.7) +
  theme_minimal() +
  labs(title = "Distribution of Packages Delivered at Each Stop",
       x = "Packages Delivered", y = "Count")

p2 <- ggplot(load_data, aes(x = RemainingPackages)) +
  geom_histogram(binwidth = 5, fill = 'darkgreen', color = 'white', alpha = 0.7) +
  theme_minimal() +
  labs(title = "Distribution of Remaining Packages",
       x = "Remaining Packages", y = "Count")

gridExtra::grid.arrange(p1, p2, ncol=2)
```

## Summary
```{r}
summary(reg_model2)
```

## Regression Analysis
```{r}
# Build linear model
reg_model <- lm(DeliveryDuration_HR ~ LoadUtilization + PackagesAtStop + RemainingPackages, data = load_data)

# Summarize model
summary(reg_model)

# Tidy output
tidy(reg_model)
```

## Correct version graph
```{r}
# Step 1: Total packages per Route
route_total_packages <- load_data %>%
  group_by(RouteID) %>%
  summarise(TotalPackages = n(), .groups = 'drop')

# Step 2: Packages Delivered at Each Stop
packages_per_stop <- load_data %>%
  group_by(RouteID, StationCode) %>%
  summarise(PackagesAtStop = n(),
            PlannedServiceTime = min(PlannedServiceTime),
            .groups = 'drop')

# Step 3: Remaining Packages
packages_ordered <- packages_per_stop %>%
  arrange(RouteID, PlannedServiceTime) %>%
  group_by(RouteID) %>%
  mutate(RemainingPackages = rev(cumsum(rev(PackagesAtStop))),
         StopOrder = row_number()) %>%
  ungroup()

head(packages_ordered)
```

## Packages Delivered
```{r}
p1 <- ggplot(packages_ordered, aes(x = PackagesAtStop)) +
  geom_histogram(binwidth = 1, fill = 'steelblue', color = 'white', alpha = 0.7) +
  theme_minimal() +
  labs(title = "Correct: Distribution of Packages Delivered at Each Stop",
       x = "Packages Delivered", y = "Count")
```

## Remaining Packages
```{r}
p2 <- ggplot(packages_ordered, aes(x = RemainingPackages)) +
  geom_histogram(binwidth = 5, fill = 'darkgreen', color = 'white', alpha = 0.7) +
  theme_minimal() +
  labs(title = "Fixed: Remaining Packages Distribution",
       x = "Remaining Packages", y = "Count")

p1
```

# Load Utilization (Correct Version)
## Load Data and Preprocessing
```{r}
# Load Data
amazondata <- read.csv("/Users/Louie/Desktop/cleaned_dataset.csv")

# Select relevant fields & Convert PlannedServiceTime to numeric if not already
load_data <- amazondata %>%
  select(RouteID, StationCode, PackageID, LoadUtilization, DeliveryDuration_HR, PlannedServiceTime) %>%
  drop_na() %>%
  mutate(PlannedServiceTime = as.numeric(PlannedServiceTime))
```

## Feature Enrichment
```{r}
# Total packages per Route
route_total_packages <- load_data %>%
  group_by(RouteID) %>%
  summarise(TotalPackages = n(), .groups = 'drop')

# Packages Delivered at Each Stop
packages_per_stop <- load_data %>%
  group_by(RouteID, StationCode) %>%
  summarise(
    PackagesAtStop     = n(),
    PlannedServiceTime = min(PlannedServiceTime),
    .groups            = 'drop'
  )

# Remaining Packages
packages_ordered <- packages_per_stop %>%
  arrange(RouteID, PlannedServiceTime) %>%
  group_by(RouteID) %>%
  mutate(
    RemainingPackages = rev(cumsum(rev(PackagesAtStop))),
    StopOrder         = row_number()
  ) %>%
  ungroup()

head(packages_ordered)

```

## Visualize the New Features
```{r}
# Packages Delivered
p1 <- ggplot(packages_ordered, aes(x = PackagesAtStop)) +
  geom_histogram(binwidth = 1, fill = 'steelblue', color = 'white', alpha = 0.7) +
  theme_minimal() +
  labs(
    title = "Correct: Distribution of Packages Delivered at Each Stop",
    x     = "Packages Delivered",
    y     = "Count"
  )

# Remaining Packages
p2 <- ggplot(packages_ordered, aes(x = RemainingPackages)) +
  geom_histogram(binwidth = 5, fill = 'darkgreen', color = 'white', alpha = 0.7) +
  theme_minimal() +
  labs(
    title = "Fixed: Remaining Packages Distribution",
    x     = "Remaining Packages",
    y     = "Count"
  )

p1
p2

```

## Merge Features for Regression Analysis
```{r}
load_data_full <- load_data %>%
  left_join(
    packages_ordered %>% select(RouteID, StationCode, PackagesAtStop, RemainingPackages),
    by = c("RouteID", "StationCode")
  ) %>%
  drop_na()

```

## Regression Modeling
```{r}
reg_model <- lm(DeliveryDuration_HR ~ LoadUtilization + PackagesAtStop, data = load_data_full)
summary(reg_model)
tidy(reg_model)
```

# Spacial Analysis (Wrong Version 1)
## Data Preparation
```{r}

amazondata <- read.csv("/Users/Louie/Desktop/cleaned_dataset.csv")
# Select necessary spatial and delivery variables
spatial_data <- amazondata %>%
  select(Latitude, Longitude, DeliveryDuration_HR) %>%
  drop_na()

# View first few rows
head(spatial_data)
```

## Basic Spacial Visualization
```{r}
# Plot all delivery points
ggplot(spatial_data, aes(x = Longitude, y = Latitude)) +
  geom_point(alpha = 0.4) +
  theme_minimal() +
  labs(title = "Amazon Delivery Locations",
       x = "Longitude", y = "Latitude")

# Color by Delivery Duration
ggplot(spatial_data, aes(x = Longitude, y = Latitude, color = DeliveryDuration_HR)) +
  geom_point(alpha = 0.6) +
  scale_color_viridis_c() +
  theme_minimal() +
  labs(title = "Delivery Duration across Locations",
       x = "Longitude", y = "Latitude", color = "Delivery Duration (hr)")
```

## K-Means Spatial Clustering
```{r}
# Choose number of clusters (K)
set.seed(123)
k_clusters <- 4
kmeans_model <- kmeans(spatial_data[, c("Latitude", "Longitude")], centers = k_clusters, nstart = 25)

# Add cluster labels to data
spatial_data$cluster <- as.factor(kmeans_model$cluster)

# View cluster counts
table(spatial_data$cluster)

# Plot Clustering Result
cluster_plot <- ggplot(spatial_data, aes(x = Longitude, y = Latitude, color = cluster)) +
  geom_point(alpha = 0.6) +
  theme_minimal() +
  labs(title = "KMeans Clustering of Delivery Locations",
       x = "Longitude", y = "Latitude", color = "Cluster")

cluster_plot
```

## Cluster-Level Delivery Analysis
```{r}
# Calculate average delivery duration per cluster
cluster_summary <- spatial_data %>%
  group_by(cluster) %>%
  summarise(
    Average_Delivery_HR = mean(DeliveryDuration_HR, na.rm = TRUE),
    Count = n()
  ) %>%
  arrange(desc(Average_Delivery_HR))

cluster_summary

# Barplot of average delivery duration by cluster
ggplot(cluster_summary, aes(x = cluster, y = Average_Delivery_HR, fill = cluster)) +
  geom_col() +
  theme_minimal() +
  labs(title = "Average Delivery Duration by Cluster",
       x = "Cluster", y = "Average Delivery Duration (hr)")
```

# Spacial Analysis (Wrong Version 2)
## Load and Prepare Data
```{r}
spatial_data <- read.csv("C:/Users/17173/Desktop/52205/cleaned_dataset.csv") %>%
  drop_na(Latitude, Longitude, DeliveryDuration_HR)

# Check state distribution
table(spatial_data$State)
```

## Delivery Points Across the US (Colored by State)
```{r}
us_map <- get_map(location = c(-98, 39), zoom = 4, scale = 2)

ggmap(us_map) +
  geom_point(data = spatial_data, aes(x = Longitude, y = Latitude, color = State),
             size = 0.7, alpha = 0.7) +
  theme_minimal() +
  labs(title = "Amazon Deliveries Across US by State",
       x = "Longitude", y = "Latitude", color = "State") +
  scale_color_brewer(palette = "Set1")
```

## Density Heatmap of Deliveries
```{r}
ggmap(us_map) +
  stat_density_2d(data = spatial_data,
                  mapping = aes(x = Longitude, y = Latitude, 
                                fill = after_stat(level), alpha = after_stat(level)),
                  geom = "polygon", bins = 30) +
  scale_fill_viridis_c() +
  scale_alpha(range = c(0.2, 0.7)) +
  labs(title = "Delivery Density Heatmap Across US") +
  theme_minimal()
```

## Focus Area: Los Angeles (CA)
```{r}
ca_data <- spatial_data %>% filter(State == "CA")

la_map <- get_map(location = c(-118.25, 34.05), zoom = 10)

ggmap(la_map) +
  geom_point(data = ca_data, aes(x = Longitude, y = Latitude), size = 0.8, alpha = 0.5) +
  labs(title = "Los Angeles Delivery Points") +
  theme_minimal()
```

## Delivery Duration Categorization in LA
```{r}
ca_data <- ca_data %>% mutate(DurationCategory = case_when(
  DeliveryDuration_HR < 0.3 ~ "Normal",
  DeliveryDuration_HR >= 0.3 & DeliveryDuration_HR <= 1 ~ "Moderate Delay",
  DeliveryDuration_HR > 1 ~ "Severe Delay"
))

ggmap(la_map) +
  geom_point(data = ca_data, aes(x = Longitude, y = Latitude, color = DurationCategory), size = 1, alpha = 0.6) +
  scale_color_manual(values = c("Normal" = "green", "Moderate Delay" = "orange", "Severe Delay" = "red")) +
  labs(title = "Delivery Stops Categorized by Delivery Duration (LA)") +
  theme_minimal()
```

## DBSCAN Clustering in LA
```{r}
coords <- ca_data %>% select(Longitude, Latitude)

set.seed(42)
db_result <- dbscan(coords, eps = 0.02, minPts = 5)
ca_data$Cluster <- as.factor(db_result$cluster)

# Remove noise points (Cluster 0)
ca_data <- ca_data %>% filter(Cluster != 0)

ggmap(la_map) +
  geom_point(data = ca_data, aes(x = Longitude, y = Latitude, color = Cluster), size = 1, alpha = 0.6) +
  labs(title = "DBSCAN Clustering of Deliveries in LA") +
  theme_minimal()

# Cluster Pain Points
cluster_summary <- ca_data %>%
  group_by(Cluster) %>%
  summarise(Avg_DeliveryDuration = mean(DeliveryDuration_HR, na.rm = TRUE),
            Count = n()) %>%
  arrange(desc(Avg_DeliveryDuration))

cluster_summary
```

## POI Analysis around LA
```{r}
poi_data <- tibble(
  Name = c("UCLA", "USC", "Downtown LA", "Santa Monica College", "California State LA", "The Grove"),
  Longitude = c(-118.4452, -118.2851, -118.2437, -118.4695, -118.1685, -118.3565),
  Latitude = c(34.0689, 34.0224, 34.0522, 34.0195, 34.0669, 34.0727),
  Type = c("School", "School", "Commercial", "School", "School", "Commercial")
)

ggmap(la_map) +
  geom_point(data = ca_data, aes(x = Longitude, y = Latitude), size = 0.5, color = "grey", alpha = 0.4) +
  geom_point(data = poi_data, aes(x = Longitude, y = Latitude, color = Type), size = 3) +
  scale_color_manual(values = c("School" = "blue", "Commercial" = "orange")) +
  labs(title = "POI Overlay and Deliveries (LA)") +
  theme_minimal()
```

## Distance to Nearest POI
```{r}
compute_nearest_poi_distance <- function(lon, lat, poi_lon, poi_lat) {
  dists <- distHaversine(cbind(lon, lat), cbind(poi_lon, poi_lat))
  return(min(dists)/1000)
}

ca_data$NearestPOI_Distance_km <- purrr::pmap_dbl(
  list(ca_data$Longitude, ca_data$Latitude),
  ~ compute_nearest_poi_distance(..1, ..2, poi_data$Longitude, poi_data$Latitude)
)

ggplot(ca_data, aes(x = NearestPOI_Distance_km, y = DeliveryDuration_HR)) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(title = "Distance to POI vs Delivery Duration (LA)",
       x = "Distance to Nearest POI (km)", y = "Delivery Duration (hr)") +
  theme_minimal()
```

## Validation with Other Cities: Seattle, Boston
```{r}
# Filter cities
seattle_data <- spatial_data %>% filter(State == "WA")
boston_data  <- spatial_data %>% filter(State == "MA")

# Map Seattle
seattle_map <- get_map(location = c(-122.3321, 47.6062), zoom = 10)

ggmap(seattle_map) +
  geom_point(data = seattle_data, aes(x = Longitude, y = Latitude), size = 0.8, alpha = 0.5) +
  labs(title = "Seattle Delivery Points") +
  theme_minimal()

# Map Boston
boston_map <- get_map(location = c(-71.0589, 42.3601), zoom = 10)

ggmap(boston_map) +
  geom_point(data = boston_data, aes(x = Longitude, y = Latitude), size = 0.8, alpha = 0.5) +
  labs(title = "Boston Delivery Points") +
  theme_minimal()
```

# Spacial Analysis (Wrong Version 3)
## National Delivery Heatmap
```{r}
spatial_data <- read.csv("C:/Users/17173/Desktop/52205/cleaned_dataset.csv") %>%
  drop_na(Longitude, Latitude, DeliveryDuration_HR)

us_map <- get_map(location = c(-98, 39), zoom = 4, scale = 2)

ggmap(us_map) +
  stat_density_2d(data = spatial_data, aes(x = Longitude, y = Latitude, fill = ..level.., alpha = ..level..),
                  geom = "polygon", bins = 30) +
  scale_fill_viridis_c() +
  scale_alpha(range = c(0.3, 0.7), guide = FALSE) +
  labs(title = "Delivery Density Heatmap Across the US") +
  theme_minimal()
```

## Delivery Point Density by City
```{r}
spatial_data <- spatial_data %>%
  mutate(CityGroup = case_when(
    State == "CA" & grepl("Los Angeles", City, ignore.case = TRUE) ~ "Los Angeles",
    State == "MA" & grepl("Boston", City, ignore.case = TRUE) ~ "Boston",
    State == "WA" & grepl("Seattle", City, ignore.case = TRUE) ~ "Seattle",
    State == "TX" & grepl("Austin", City, ignore.case = TRUE) ~ "Austin",
    State == "IL" & grepl("Chicago", City, ignore.case = TRUE) ~ "Chicago",
    TRUE ~ NA_character_
  )) %>% filter(!is.na(CityGroup))

cities <- unique(spatial_data$CityGroup)
for (city in cities) {
  city_data <- spatial_data %>% filter(CityGroup == city)
  center <- colMeans(city_data[, c("Longitude", "Latitude")])
  map <- get_map(location = center, zoom = 11)
  print(
    ggmap(map) +
      geom_point(data = city_data, aes(x = Longitude, y = Latitude), size = 0.7, alpha = 0.5) +
      labs(title = paste(city, "Delivery Point Density")) +
      theme_minimal()
  )
}
```

## Delay Hotspots in 5 Cities (Quantile Binning)
```{r}
for (city in cities) {
  city_data <- spatial_data %>% filter(CityGroup == city)
  city_data <- city_data %>% mutate(DelayMin = DeliveryDuration_HR * 60)
  qtiles <- quantile(city_data$DelayMin, probs = seq(0, 1, 0.2), na.rm = TRUE)
  if (length(unique(qtiles)) < length(qtiles)) qtiles <- jitter(qtiles, amount = 0.01)
  city_data$delay_bin <- cut(city_data$DelayMin, breaks = qtiles, labels = c("1", "2", "3", "4", "5"), include.lowest = TRUE)
  center <- colMeans(city_data[, c("Longitude", "Latitude")])
  map <- get_map(location = center, zoom = 13, maptype = "roadmap")
  print(
    ggmap(map) +
      geom_point(data = city_data, aes(x = Longitude, y = Latitude, color = delay_bin), size = 1) +
      scale_color_manual(values = rev(brewer.pal(5, "YlOrRd")), na.value = "grey") +
      labs(title = paste(city, "Delay Hotspots"), subtitle = "5-Quantile Delay Time", color = "Delay (min)") +
      theme_minimal()
  )
}
```

## POI Overlay by City (Commercial & School Zones)
```{r}
# This is a placeholder - in production, use real school database / Google Places API results
poi_data <- tibble(
  Name = c("UCLA", "Downtown LA", "USC", "MIT", "Harvard", "Logan Airport", "Navy Pier", "UT Austin"),
  Longitude = c(-118.4452, -118.2437, -118.2851, -71.0921, -71.1167, -71.0202, -87.6091, -97.7431),
  Latitude = c(34.0689, 34.0522, 34.0224, 42.3601, 42.3770, 42.3656, 41.8916, 30.2849),
  Type = c("School", "Commercial", "School", "School", "School", "Commercial", "Commercial", "School")
)

for (city in cities) {
  city_data <- spatial_data %>% filter(CityGroup == city)
  center <- colMeans(city_data[, c("Longitude", "Latitude")])
  map <- get_map(location = center, zoom = 13)
  print(
    ggmap(map) +
      geom_point(data = city_data, aes(x = Longitude, y = Latitude), color = "grey", alpha = 0.3, size = 1) +
      geom_point(data = poi_data, aes(x = Longitude, y = Latitude, color = Type), size = 3) +
      scale_color_manual(values = c("School" = "blue", "Commercial" = "orange")) +
      labs(title = paste("POI Overlay -", city)) +
      theme_minimal()
  )
}
```

## Load Utilization Spatial Analysis
```{r}
if (!"LoadUtilization" %in% names(spatial_data)) {
  set.seed(1234)
  spatial_data$LoadUtilization <- runif(nrow(spatial_data), 0.5, 1)
}

for (city in cities) {
  city_data <- spatial_data %>% filter(CityGroup == city)
  center <- colMeans(city_data[, c("Longitude", "Latitude")])
  map <- get_map(location = center, zoom = 12)
  print(
    ggmap(map) +
      geom_point(data = city_data, aes(x = Longitude, y = Latitude, color = LoadUtilization), size = 1.2) +
      scale_color_viridis_c(option = "inferno") +
      labs(title = paste(city, "Load Utilization Map"), color = "Load %") +
      theme_minimal()
  )
}
```

## Peak Hour Delivery Density
```{r}
# Ensure Timestamp exists and is parsable
if ("Timestamp" %in% colnames(spatial_data)) {
  parsed_times <- suppressWarnings(as.POSIXct(spatial_data$Timestamp))
  if (any(!is.na(parsed_times))) {
    spatial_data$Hour <- as.numeric(format(parsed_times, "%H"))
    
    spatial_data <- spatial_data %>%
      mutate(TimePeriod = case_when(
        Hour >= 7 & Hour <= 10 ~ "Morning Peak",
        Hour >= 16 & Hour <= 19 ~ "Evening Peak",
        TRUE ~ "Other"
      ))

    for (period in c("Morning Peak", "Evening Peak")) {
      peak_data <- spatial_data %>% filter(TimePeriod == period)
      if (nrow(peak_data) > 0) {
        center <- colMeans(peak_data[, c("Longitude", "Latitude")])
        map <- get_map(location = center, zoom = 4)
        print(
          ggmap(map) +
            stat_density_2d(data = peak_data, aes(x = Longitude, y = Latitude, fill = ..level.., alpha = ..level..),
                            geom = "polygon", bins = 30) +
            scale_fill_viridis_c() +
            scale_alpha(range = c(0.3, 0.7), guide = FALSE) +
            labs(title = paste("Peak Hour Delivery Density -", period)) +
            theme_minimal()
        )
      }
    }
  } else {
    warning("Timestamp column exists but could not be parsed. Skipping peak hour analysis.")
  }
} else {
  warning("No Timestamp column found in data. Skipping peak hour analysis.")
}
```

# Spacial Analysis (Wrong Version 4)
## Register Google API key
```{r}
ggmap::register_google(key = 'AIzaSyD2H7Xy0yI1hEONVTzQciE-D5cRFSlOIZw')
```

## Point‐based Hotspot Plot (binned DelayedMins)
```{r}
plot_point_hotspots <- function(df, city, zoom = 12) {
  sub <- df %>% filter(City == city)
  if (nrow(sub)==0) stop("No data for ", city)
  
  # assign delayed time into 5 quantile bins
  sub <- sub %>%
    mutate(delay_bin = factor(ntile(DelayedMins, 5)))
  
  # bounding box and basemap
  bb       <- make_bbox(lon = sub$Longitude, lat = sub$Latitude, f = 0.1)
  base_map <- get_map(location = bb, source = 'google', maptype = 'roadmap', zoom = zoom)
  
  ggmap(base_map) +
    geom_point(
      data  = sub,
      aes(x = Longitude, y = Latitude, color = delay_bin),
      size  = 0.5,
      alpha = 0.7
    ) +
    scale_color_brewer(
      type      = 'seq',
      palette   = 'YlOrRd',   # yellow (small delays) -> red (large delays)
      direction = 1,
      name      = 'Delayed\nTime (mins)'
    ) +
    labs(
      title    = paste(city, "Delivery Hotspots"),
      subtitle = "Stops colored by 5 quantile bins of delayed time"
    ) +
    theme_minimal()
}

# Plot for five metros
cities <- c("Los Angeles","Anaheim","Boston","Seattle","Chicago")
zooms  <- c(11,            12,         12,       11,        11)
for (i in seq_along(cities)) {
  print(plot_point_hotspots(delivery_df, cities[i], zoom = zooms[i]))
}
```

## Spatio‐Temporal DBSCAN
```{r}
st_feats  <- delivery_df %>% select(Longitude, Latitude, hour) %>% as.matrix()
st_scaled <- scale(st_feats)
db        <- dbscan(st_scaled, eps = 0.5, minPts = 20)
delivery_df$cluster <- factor(db$cluster)

# Summarize cluster delay & peak time
cluster_stats <- delivery_df %>%
  filter(cluster != 0) %>%     # drop noise
  group_by(cluster) %>%
  summarise(
    n_stops   = n(),
    avg_delay = mean(DelayedMins, na.rm = TRUE),
    peak_hour = hour[which.max(table(cut(hour, breaks = 0:24)))]
  ) %>%
  arrange(desc(avg_delay))
print(cluster_stats)

# Map top 3 congested clusters
top3 <- cluster_stats %>% slice(1:3) %>% pull(cluster)

ggplot(delivery_df %>% filter(cluster %in% top3),
       aes(x = Longitude, y = Latitude, color = cluster)) +
  geom_point(alpha = 0.8, size = 1.5) +
  labs(
    title    = "Top 3 Congestion‐Prone Clusters",
    subtitle = "by Avg. Delayed Time",
    color    = "Cluster ID"
  ) +
  theme_minimal()
```

## Temporal Delay Profile
```{r}
hourly_summary <- delivery_df %>%
  group_by(hr = floor(hour)) %>%
  summarise(
    avg_delay = mean(DelayedMins, na.rm = TRUE),
    stops     = n()
  )

ggplot(hourly_summary, aes(x = hr, y = avg_delay)) +
  geom_line(size = 1) +
  geom_point() +
  scale_x_continuous(breaks = 0:23) +
  labs(
    title = "Average Delayed Time by Hour of Day",
    x     = "Hour of Day",
    y     = "Avg. Delayed Time (mins)"
  ) +
  theme_minimal()
```

# Spacial Analysis (Correct Version)
## Data Loading & Initial Cleaning
```{r}
# Load & Prepare Data
spatial_data <- read.csv("/Users/Louie/Desktop/cleaned_dataset.csv") %>%
  drop_na(Latitude, Longitude, DeliveryDuration_HR, PlannedServiceTime) %>%
  mutate(
    DeliveryDelay_HR = as.numeric(
      difftime(as.POSIXct(EndTimeUTC), as.POSIXct(StartTimeUTC), units = "hours")
    ) - (PlannedServiceTime / 3600),
    DeliveryHour   = hour(as.POSIXct(StartTimeUTC)),
    DeliveryPeriod = case_when(
      DeliveryHour < 12 ~ "Morning",
      DeliveryHour < 18 ~ "Afternoon",
      TRUE             ~ "Evening"
    )
  )
```

## Define Focus Cities
```{r}
cities <- tibble(
  city = c("Los Angeles", "Seattle", "Boston", "Austin", "Chicago"),
  lon  = c(-118.25,      -122.33,    -71.06,    -97.74,   -87.63),
  lat  = c(34.05,         47.61,      42.36,     30.27,    41.88)
)
```

## City‐level Hotspot Plots
```{r}
plot_city_hotspots <- function(df, city_name, center_lon, center_lat,
                               extent = 0.5, zoom = 10) {
  sub <- df %>%
    filter(
      Longitude >= center_lon - extent,
      Longitude <= center_lon + extent,
      Latitude  >= center_lat - extent,
      Latitude  <= center_lat + extent
    )
  if (nrow(sub) == 0) stop("No data for ", city_name)
  
  bbox <- c(
    left   = center_lon - extent,
    bottom = center_lat - extent,
    right  = center_lon + extent,
    top    = center_lat + extent
  )
  base_map <- get_map(location = bbox, source = "google", maptype = "roadmap", zoom = zoom)
  
  ggmap(base_map) +
    geom_point(
      data = sub,
      aes(x = Longitude, y = Latitude,
          color = pmin(pmax(DeliveryDelay_HR, 0), 5)),
      size = 0.5, alpha = 0.7
    ) +
    scale_color_gradientn(
      colors = rev(brewer.pal(11, "RdYlGn")),
      limits = c(0, 5),
      name   = "Delay (hrs)"
    ) +
    labs(
      title    = paste(city_name, "Delivery Hotspots"),
      subtitle = "Stops colored by delay (0–5 hrs)"
    ) +
    theme_minimal()
}

# generate and print hotspot maps for each city
for (i in seq_len(nrow(cities))) {
  print(
    plot_city_hotspots(
      spatial_data,
      cities$city[i],
      cities$lon[i],
      cities$lat[i]
    )
  )
}
```

## Identify Top-20 Clusters via DBSCAN
```{r}
# Extract coordinates & run DBSCAN clustering
coords <- spatial_data %>%
  select(Longitude, Latitude) %>%
  mutate(
    Longitude = as.numeric(Longitude),
    Latitude  = as.numeric(Latitude)
  ) %>%
  data.matrix()

# Run DBSCAN with eps = 0.05, minPts = 30
set.seed(42)
db <- dbscan::dbscan(coords, eps = 0.05, minPts = 30)

# Assign cluster labels
spatial_data$Cluster <- factor(db$cluster)

# Summarize top-20 clusters by delivery count
cluster_summary <- spatial_data %>%
  filter(Cluster != "0") %>%
  group_by(Cluster) %>%
  summarise(
    DeliveryCount = n(),
    Lon           = mean(Longitude),
    Lat           = mean(Latitude),
    .groups       = "drop"
  ) %>%
  arrange(desc(DeliveryCount)) %>%
  slice(1:20) %>%
  mutate(ClusterRank = row_number())
```

## Join Cluster Ranks & Build Color Palette
```{r}
spatial_data <- spatial_data %>%
  left_join(cluster_summary %>% select(Cluster, ClusterRank), by = "Cluster") %>%
  filter(!is.na(ClusterRank))

cluster_palette <- colorRampPalette(brewer.pal(11, "RdYlGn"))(20)
```

## Nationwide Map of Top-20 Clusters
```{r}
# Use US map centered on the continental US for a broad view
us_map <- get_map(location = c(-98, 39), zoom = 4, scale = 2, source = "google")

# Plot the map and overlay our top-20 delivery clusters
ggmap(us_map) +
  geom_point(
    data  = cluster_summary,
    aes(x = Lon, y = Lat, size = DeliveryCount, color = factor(ClusterRank)),
    alpha = 0.7
  ) +
  geom_label_repel(
    data = cluster_summary,
    aes(x = Lon, y = Lat, label = ClusterRank),
    size = 3, box.padding = 0.3
  ) +
  scale_color_manual(name = "Cluster Rank", values = cluster_palette) +
  scale_size(range = c(2, 10)) +
  theme_minimal() +
  labs(title = "Top 20 Delivery Clusters Across the US")
```

## Delay Composition per Cluster
```{r}
# Categorize each delivery stop’s delay into On Time / Slight / Moderate / Severe
spatial_data <- spatial_data %>%
  mutate(
    DelayCategory = factor(
      case_when(
        DeliveryDelay_HR <= 0 ~ "On Time",
        DeliveryDelay_HR <= 1 ~ "Slight",
        DeliveryDelay_HR <= 5 ~ "Moderate",
        TRUE                 ~ "Severe"
      ),
      levels = c("On Time", "Slight", "Moderate", "Severe")
    )
  )

# Calculate counts and share of each delay category within each cluster rank
delay_by_cluster <- spatial_data %>%
  group_by(ClusterRank, DelayCategory) %>%
  summarise(n = n(), .groups = "drop") %>%
  group_by(ClusterRank) %>%
  mutate(share = n / sum(n))

# Order clusters by their “On Time” share (descending)
on_time_order <- delay_by_cluster %>%
  filter(DelayCategory == "On Time") %>%
  arrange(desc(share)) %>%
  pull(ClusterRank)

# Plot a 100% stacked bar chart of delay composition, ordered by on-time performance
ggplot(delay_by_cluster, aes(
  x    = factor(ClusterRank, levels = on_time_order),
  y    = share,
  fill = DelayCategory
)) +
  geom_col(position = "fill", color = "white") +
  scale_fill_brewer(name = "Delay Category", palette = "RdYlGn", direction = -1) +
  theme_minimal() +
  labs(
    title = "Delay Composition per Cluster (Ordered by “On Time” Share)",
    x     = "Cluster Rank",
    y     = "Proportion of Deliveries"
  )
```

## Average Delivery Delay by Time Period
```{r}
# Compute the average delivery delay for each cluster by time period
delay_period <- spatial_data %>%
  group_by(ClusterRank, DeliveryPeriod) %>%
  summarise(AvgDelay = mean(DeliveryDelay_HR, na.rm = TRUE), .groups = "drop")

# Plot average delivery delay by time period, side-by-side for each cluster
ggplot(delay_period, aes(x = factor(ClusterRank), y = AvgDelay, fill = DeliveryPeriod)) +
  geom_col(position = "dodge") +
  theme_minimal() +
  labs(
    title = "Average Delivery Delay by Time Period",
    x     = "Cluster Rank",
    y     = "Avg Delay (hrs)"
  )
```

## Load Utilization Distribution by Cluster
```{r}
ggplot(spatial_data, aes(x = factor(ClusterRank), y = LoadUtilization)) +
  geom_violin(fill = "lightblue", color = "black", alpha = 0.6) +
  geom_jitter(width = 0.2, alpha = 0.2, size = 0.5) +
  theme_minimal() +
  labs(
    title = "Load Utilization by Cluster",
    x     = "Cluster Rank",
    y     = "Load Utilization (1–6)"
  )
```

## Cluster-Level Risk Scoring & Summary Table
```{r}
# Summarize key metrics for each of the top clusters
cluster_profile <- spatial_data %>%
  filter(Cluster %in% cluster_summary$Cluster) %>%
  group_by(Cluster) %>%
  summarise(
    Orders     = n(),
    AvgDelay   = mean(DeliveryDelay_HR, na.rm = TRUE),
    SeverePct  = mean(DelayCategory == "Severe"),
    AvgUtil    = mean(LoadUtilization, na.rm = TRUE),
    Efficiency = mean(DistanceToNextStop / DeliveryDuration_HR, na.rm = TRUE),
    .groups    = "drop"
  ) %>%
  mutate(
    DelayScore = ntile(AvgDelay, 5),
    UtilScore  = 6 - ntile(AvgUtil, 5),
    EffScore   = 6 - ntile(Efficiency, 5),
    RiskScore  = DelayScore + UtilScore + EffScore,
    RiskLevel  = case_when(
      RiskScore >= 13 ~ "🔴 High Risk",
      RiskScore >= 10 ~ "🟠 Moderate Risk",
      TRUE            ~ "🟢 Low Risk"
    )
  ) %>%
  arrange(desc(RiskScore))

# Render the cluster-level risk table with styling
cluster_profile %>%
  kbl(caption = "Cluster-Level Risk Table (Quantile-Based Scoring)") %>%
  kable_styling(full_width = FALSE) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(7, color = "white", background = "#FF6961")
```

## System-Wide Delivery Delay & Load Utilization Distributions
```{r}
# Plot the overall distribution of delivery delays across all stops
ggplot(spatial_data, aes(x = DeliveryDelay_HR)) +
  geom_density(fill = "lightcoral", alpha = 0.6) +
  theme_minimal() +
  labs(title = "Overall Distribution of Delivery Delay (Hours)")
```

# Resequencing (Wrong Version 1)
## Load and Prepare Data
```{r}
spatial_data <- read.csv("C:/Users/17173/Desktop/52205/cleaned_dataset.csv") %>%
  drop_na(Latitude, Longitude, DeliveryDuration_HR)

ca_data <- spatial_data %>% filter(State == "CA")

coords <- ca_data %>% select(Longitude, Latitude)
set.seed(42)
db_result <- dbscan(coords, eps = 0.02, minPts = 5)
ca_data$Cluster <- as.factor(db_result$cluster)

cluster_sizes <- ca_data %>%
  group_by(Cluster) %>%
  summarise(count = n())

selected_cluster <- cluster_sizes %>%
  filter(count >= 20) %>%
  arrange(desc(count)) %>%
  slice(1) %>%
  pull(Cluster)

target_data <- ca_data %>% filter(Cluster == selected_cluster)
```

## Filter Stops by Load Utilization Range
```{r}
if (!"LoadUtilization" %in% names(target_data)) {
  set.seed(123)
  target_data$LoadUtilization <- runif(nrow(target_data), 0.5, 1)
}

target_data <- target_data %>%
  filter(LoadUtilization >= 0.6, LoadUtilization <= 0.9)
```

## Prepare Base Map
```{r}
la_map <- get_map(location = c(lon = -118.25, lat = 34.05), zoom = 11, source = "google", maptype = "roadmap")
```

## Baseline Route Visualization (Original Stop Order)
```{r}
original_route_plot <- ggmap(la_map) +
  geom_point(data = target_data, aes(x = Longitude, y = Latitude), size = 2, color = "blue") +
  geom_path(data = target_data, aes(x = Longitude, y = Latitude), color = "blue", linetype = "dashed") +
  labs(title = "Original Delivery Sequence (Selected Cluster)") +
  theme_minimal()
```

## Resequencing Using Nearest Neighbor (TSP Solver)
```{r}
if(nrow(target_data) >= 4) {
  distance_matrix <- distm(target_data %>% select(Longitude, Latitude))
  tsp <- TSP(as.dist(distance_matrix))
  solution <- solve_TSP(tsp)
  solution <- as.integer(solution)
  target_data_resequenced <- target_data[solution, ]
} else {
  warning("Not enough delivery points to resequence. Keeping original order.")
  target_data_resequenced <- target_data
}
```

## Visualization After Resequencing
```{r}
resequenced_route_plot <- ggmap(la_map) +
  geom_point(data = target_data_resequenced, aes(x = Longitude, y = Latitude), size = 2, color = "red") +
  geom_path(data = target_data_resequenced, aes(x = Longitude, y = Latitude), color = "red") +
  labs(title = "Resequenced Delivery Route (Selected Cluster)") +
  theme_minimal()

if (nrow(target_data) >= 2 && nrow(target_data_resequenced) >= 2) {
  grid.arrange(original_route_plot, resequenced_route_plot, ncol = 2)
} else {
  warning("Not enough stops to plot both routes.")
}
```

## Efficiency Evaluation
```{r}
if(nrow(target_data) >= 2 && nrow(target_data_resequenced) >= 2){
  original_coords <- as.matrix(target_data %>% select(Longitude, Latitude))
  reseq_coords <- as.matrix(target_data_resequenced %>% select(Longitude, Latitude))

  original_distance <- sum(distHaversine(
    original_coords[-nrow(original_coords), ],
    original_coords[-1, ]
  ), na.rm = TRUE)

  reseq_distance <- sum(distHaversine(
    reseq_coords[-nrow(reseq_coords), ],
    reseq_coords[-1, ]
  ), na.rm = TRUE)

  improvement_rate <- round((original_distance - reseq_distance) / original_distance * 100, 2)

  efficiency_result <- data.frame(
    Metric = c("Original Total Distance (m)", "Resequenced Total Distance (m)", "Improvement %"),
    Value = c(original_distance, reseq_distance, improvement_rate)
  )
  
  knitr::kable(efficiency_result, caption = "Efficiency Comparison Before and After Resequencing")
  
  cat("\n\n**Summary:** Total travel distance reduced by ", improvement_rate, "% after resequencing.\n")
} else {
  warning("Not enough stops to evaluate efficiency.")
}
```


# Resequencing (Wrong Version 2) top 20 rmd
## Load and Prepare Data
```{r}
spatial_data <- read.csv("C:/Users/17173/Desktop/52205/cleaned_dataset.csv") %>%
  drop_na(Latitude, Longitude, DeliveryDuration_HR, PlannedServiceTime)

spatial_data$DeliveryDelay_HR <- as.numeric(difftime(as.POSIXct(spatial_data$EndTimeUTC), as.POSIXct(spatial_data$StartTimeUTC), units = "hours")) - (spatial_data$PlannedServiceTime / 3600)

spatial_data <- spatial_data %>%
  mutate(DeliveryHour = hour(as.POSIXct(StartTimeUTC)),
         DeliveryPeriod = case_when(
           DeliveryHour < 12 ~ "Morning",
           DeliveryHour < 18 ~ "Afternoon",
           TRUE ~ "Evening"
         ))
```

## Identify Top 20 High-Density Clusters
```{r}
coords <- spatial_data %>% select(Longitude, Latitude) %>% drop_na() %>% as.matrix()
set.seed(42)
db <- dbscan(coords, eps = 0.05, minPts = 30)
spatial_data$Cluster <- as.factor(db$cluster)

cluster_summary <- spatial_data %>%
  filter(Cluster != 0) %>%
  group_by(Cluster) %>%
  summarise(
    DeliveryCount = n(),
    Lon = mean(Longitude),
    Lat = mean(Latitude)
  ) %>%
  arrange(desc(DeliveryCount)) %>%
  slice(1:20)

write.csv(spatial_data, "C:/Users/17173/Desktop/52205/spatial_clustered_nationally.csv", row.names = FALSE)
```

## Visualize Top Clusters (Nationwide and 5 Focus Cities)
```{r}
us_map <- get_map(location = c(-98, 39), zoom = 4, scale = 2)

ggmap(us_map) +
  geom_point(data = cluster_summary, aes(x = Lon, y = Lat, size = DeliveryCount, color = Cluster), alpha = 0.7) +
  geom_label_repel(data = cluster_summary, aes(x = Lon, y = Lat, label = Cluster), size = 3, box.padding = 0.3) +
  scale_size(range = c(2, 10)) +
  theme_minimal() +
  labs(title = "Top 20 Delivery Clusters Across US")

cities <- tibble(
  city = c("Los Angeles", "Seattle", "Boston", "Austin", "Chicago"),
  lon = c(-118.25, -122.33, -71.06, -97.74, -87.63),
  lat = c(34.05, 47.61, 42.36, 30.27, 41.88)
)

for (i in 1:nrow(cities)) {
  city_name <- cities$city[i]
  city_map <- get_map(location = c(cities$lon[i], cities$lat[i]), zoom = 10)
  p <- ggmap(city_map) +
    geom_point(data = spatial_data %>% filter(Cluster %in% cluster_summary$Cluster),
               aes(x = Longitude, y = Latitude, color = Cluster), alpha = 0.6, size = 1.2) +
    labs(title = paste("Cluster Distribution in", city_name)) +
    theme_minimal()
  print(p)
}
```

## Cluster Problem Diagnosis
```{r}
# Delay Composition (Stacked Bar)
spatial_data <- spatial_data %>%
  mutate(DelayCategory = case_when(
    DeliveryDelay_HR <= 0 ~ "On Time",
    DeliveryDelay_HR <= 1 ~ "Slight",
    DeliveryDelay_HR <= 5 ~ "Moderate",
    DeliveryDelay_HR > 5 ~ "Severe"
  ))

delay_by_cluster <- spatial_data %>%
  filter(Cluster %in% cluster_summary$Cluster) %>%
  group_by(Cluster, DelayCategory) %>%
  summarise(n = n()) %>%
  group_by(Cluster) %>%
  mutate(share = n / sum(n))

ggplot(delay_by_cluster, aes(x = fct_reorder(Cluster, -share), y = share, fill = DelayCategory)) +
  geom_col(position = 'fill', color = "white") +
  theme_minimal() +
  labs(title = "Delay Composition per Cluster", y = "Share", x = "Cluster") +
  scale_fill_brewer(palette = "Paired")

# Delay by Time Period
delay_period <- spatial_data %>%
  filter(Cluster %in% cluster_summary$Cluster) %>%
  group_by(Cluster, DeliveryPeriod) %>%
  summarise(AvgDelay = mean(DeliveryDelay_HR, na.rm = TRUE))

ggplot(delay_period, aes(x = Cluster, y = AvgDelay, fill = DeliveryPeriod)) +
  geom_col(position = "dodge") +
  theme_minimal() +
  labs(title = "Avg Delivery Delay by Time Period")

# Load Utilization Analysis
ggplot(spatial_data %>% filter(Cluster %in% cluster_summary$Cluster),
       aes(x = Cluster, y = LoadUtilization)) +
  geom_violin(fill = "lightblue", color = "black", alpha = 0.6) +
  geom_jitter(width = 0.2, alpha = 0.2, size = 0.5) +
  theme_minimal() +
  labs(title = "Load Utilization by Cluster")

# Risk Scoring & Summary Table (Weighted by Cluster Size)
cluster_profile <- spatial_data %>%
  filter(Cluster %in% cluster_summary$Cluster) %>%
  group_by(Cluster) %>%
  summarise(
    Orders = n(),
    AvgDelay = mean(DeliveryDelay_HR, na.rm = TRUE),
    SeverePct = mean(DelayCategory == "Severe"),
    AvgUtil = mean(LoadUtilization, na.rm = TRUE),
    Efficiency = mean(DistanceToNextStop / DeliveryDuration_HR, na.rm = TRUE)
  ) %>%
  mutate(
    DelayScore = ntile(AvgDelay, 5),
    UtilScore = 6 - ntile(AvgUtil, 5),
    EffScore = 6 - ntile(Efficiency, 5),
    RiskScore = DelayScore + UtilScore + EffScore,
    RiskLevel = case_when(
      RiskScore >= 13 ~ "🔴 High Risk",
      RiskScore >= 10 ~ "🟠 Moderate Risk",
      TRUE ~ "🟢 Low Risk"
    )
  ) %>%
  arrange(desc(RiskScore))

cluster_profile %>%
  kbl(caption = "Cluster-Level Risk Table (Quantile-Based Scoring)") %>%
  kable_styling(full_width = F) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(7, color = "white", background = "#FF6961")
```

## System-Wide Diagnosis
```{r}
# Delay
ggplot(spatial_data, aes(x = DeliveryDelay_HR)) +
  geom_density(fill = "lightcoral", alpha = 0.6) +
  theme_minimal() +
  labs(title = "Overall Distribution of Delivery Delay (Hours)")

# Load Utilization
ggplot(spatial_data, aes(x = LoadUtilization)) +
  geom_density(fill = "skyblue", alpha = 0.6) +
  theme_minimal() +
  labs(title = "Overall Distribution of Load Utilization")
```

# Resequencing (Wrong Version 3)
## Load and Filter High-Risk Clusters from National Spatial Analysis
```{r}
spatial_data <- read.csv("C:/Users/17173/Desktop/52205/spatial_clustered_national.csv") %>%
  drop_na(Latitude, Longitude, DeliveryDuration_HR)

# Select top 3 national high-risk clusters (e.g., Cluster 26, 8, 1)
target_clusters <- c(15, 20, 1)
filtered_data <- spatial_data %>% filter(Cluster %in% target_clusters)

coords <- filtered_data %>% select(Longitude, Latitude)
set.seed(42)
# Retain original national cluster labels
filtered_data$Cluster <- as.factor(filtered_data$Cluster)


# Identify top 3 largest dense clusters
top_clusters <- unique(filtered_data$Cluster)
```

## Define Resequencing Function and Evaluation
```{r}
resequence_weighted <- function(df) {
  coords <- df %>% select(Longitude, Latitude)
  dist_matrix <- distm(coords, fun = distHaversine)
  time_weights <- df$PredictedTime
  n <- nrow(df)
  path <- numeric(n)
  visited <- rep(FALSE, n)
  path[1] <- 1
  visited[1] <- TRUE
  for (i in 2:n) {
    last <- path[i - 1]
    cost_vec <- dist_matrix[last, ] * time_weights
    cost_vec[visited] <- Inf
    next_stop <- which.min(cost_vec)
    path[i] <- next_stop
    visited[next_stop] <- TRUE
  }
  df[path, ]
}

resequence_greedy <- function(df) {
  coords <- df %>% select(Longitude, Latitude)
  dist_matrix <- distm(coords, fun = distHaversine)
  n <- nrow(df)
  path <- numeric(n)
  visited <- rep(FALSE, n)
  path[1] <- 1
  visited[1] <- TRUE
  for (i in 2:n) {
    last <- path[i - 1]
    cost_vec <- dist_matrix[last, ]
    cost_vec[visited] <- Inf
    next_stop <- which.min(cost_vec)
    path[i] <- next_stop
    visited[next_stop] <- TRUE
  }
  df[path, ]
}
```

## Iterate Over Top Clusters, Visualize and Compare Results
```{r}
summary_results <- list()

for (cl in top_clusters) {
  target_data <- filtered_data %>% filter(Cluster == cl)

  if (!"LoadUtilization" %in% names(target_data)) {
    set.seed(123)
    target_data$LoadUtilization <- runif(nrow(target_data), 0.5, 1)
  }
  target_data <- target_data %>%
    filter(LoadUtilization >= 0.3, LoadUtilization <= 0.9) %>%
    mutate(PackagesAtStop = sample(1:10, nrow(.), replace = TRUE),
           PredictedTime = 9.3 - 1.17 * LoadUtilization + 0.005 * PackagesAtStop)

  if (nrow(target_data) < 4) next

  original_coords <- target_data %>% select(Longitude, Latitude)
  greedy_data <- resequence_greedy(target_data)
  weighted_data <- resequence_weighted(target_data)

  greedy_coords <- greedy_data %>% select(Longitude, Latitude)
  weighted_coords <- weighted_data %>% select(Longitude, Latitude)

  d0 <- sum(distHaversine(original_coords[-nrow(original_coords), ], original_coords[-1, ]))
  d1 <- sum(distHaversine(greedy_coords[-nrow(greedy_coords), ], greedy_coords[-1, ]))
  d2 <- sum(distHaversine(weighted_coords[-nrow(weighted_coords), ], weighted_coords[-1, ]))

  summary_results[[as.character(cl)]] <- tibble(
    Cluster = cl,
    Original_Dist = round(d0, 2),
    Greedy_Dist = round(d1, 2),
    Weighted_Dist = round(d2, 2),
    Greedy_Improve = round((d0 - d1) / d0 * 100, 2),
    Weighted_Improve = round((d0 - d2) / d0 * 100, 2)
  )
}

final_summary <- bind_rows(summary_results)

# Merge with risk score table if available
if ("RiskScore" %in% names(filtered_data)) {
  risk_summary <- filtered_data %>%
    filter(Cluster %in% top_clusters) %>%
    group_by(Cluster) %>%
    summarise(
      AvgDelay = mean(DeliveryDelay_HR, na.rm = TRUE),
      AvgUtil = mean(LoadUtilization, na.rm = TRUE),
      RiskScore = mean(RiskScore, na.rm = TRUE),
      ProblemType = first(ProblemType)
    )

  final_summary <- final_summary %>%
    left_join(risk_summary, by = "Cluster")
}

# Generate Map for Each Cluster (Original + Greedy + Weighted)
for (cl in top_clusters) {
  cluster_data <- filtered_data %>% filter(Cluster == cl)
  if (nrow(cluster_data) < 4) next

  cluster_data <- cluster_data %>%
    filter(LoadUtilization >= 0.6, LoadUtilization <= 0.9) %>%
    mutate(PackagesAtStop = sample(1:10, nrow(.), replace = TRUE),
           PredictedTime = 9.3 - 1.17 * LoadUtilization + 0.005 * PackagesAtStop)

  greedy_data <- resequence_greedy(cluster_data)
  weighted_data <- resequence_weighted(cluster_data)

  map_center <- c(lon = mean(cluster_data$Longitude), lat = mean(cluster_data$Latitude))

  original_data <- cluster_data %>% arrange(row_number())
  basemap <- get_map(location = map_center, zoom = 12)

  p1 <- ggmap(basemap) +
    geom_path(data = original_data, aes(x = Longitude, y = Latitude), color = "gray", linetype = "dotted") +
    geom_point(data = original_data, aes(x = Longitude, y = Latitude), color = "gray", size = 2) +
    geom_path(data = greedy_data, aes(x = Longitude, y = Latitude), color = "blue", linetype = "dashed") +
    geom_point(data = greedy_data, aes(x = Longitude, y = Latitude), color = "blue", size = 2) +
    labs(title = paste("Cluster", cl, "- Greedy Resequencing vs Original")) +
    theme_minimal()

  p2 <- ggmap(basemap) +
    geom_path(data = original_data, aes(x = Longitude, y = Latitude), color = "gray", linetype = "dotted") +
    geom_point(data = original_data, aes(x = Longitude, y = Latitude), color = "gray", size = 2) +
    geom_path(data = weighted_data, aes(x = Longitude, y = Latitude), color = "red") +
    geom_point(data = weighted_data, aes(x = Longitude, y = Latitude), color = "red", size = 2) +
    labs(title = paste("Cluster", cl, "- Weighted Resequencing vs Original")) +
    theme_minimal()

  library(ggrepel)

  p1 <- p1 +
    geom_text_repel(data = greedy_data %>% mutate(idx = row_number()),
                    aes(x = Longitude, y = Latitude, label = idx), size = 2.5)

  p2 <- p2 +
    geom_text_repel(data = weighted_data %>% mutate(idx = row_number()),
                    aes(x = Longitude, y = Latitude, label = idx), size = 2.5)

  p0 <- ggmap(basemap) +
    geom_path(data = original_data, aes(x = Longitude, y = Latitude), color = "black", linetype = "solid") +
    geom_point(data = original_data, aes(x = Longitude, y = Latitude), color = "black", size = 2) +
    geom_text_repel(data = original_data %>% mutate(idx = row_number()),
                    aes(x = Longitude, y = Latitude, label = idx), size = 2.5) +
    labs(title = paste("Cluster", cl, "- Original Route")) +
    theme_minimal()

  grid.arrange(p0, p1, p2, ncol = 3)
}

knitr::kable(final_summary, caption = "Resequencing Impact Across Top Clusters")
```

# Resequencing (Wrong Version 4) resequencing.rmd
## Register Google Maps API key
```{r}
register_google(key = "AIzaSyD2H7Xy0yI1hEONVTzQciE-D5cRFSlOIZw")
```

## Load & prepare data
```{r}
spatial_data <- read.csv("/Users/Louie/Desktop/cleaned_dataset.csv") %>%
  drop_na(Latitude, Longitude, DeliveryDuration_HR, PlannedServiceTime) %>%
  mutate(
    DeliveryDelay_HR = as.numeric(
      difftime(as.POSIXct(EndTimeUTC), as.POSIXct(StartTimeUTC), units = "hours")
    ) - (PlannedServiceTime / 3600),
    DeliveryHour = hour(as.POSIXct(StartTimeUTC)),
    DeliveryPeriod = case_when(
      DeliveryHour < 12 ~ "Morning",
      DeliveryHour < 18 ~ "Afternoon",
      TRUE              ~ "Evening"
    )
  )
```

## Identify top-20 clusters via DBSCAN
```{r}
coords <- spatial_data %>% select(Longitude, Latitude) %>% as.matrix()
set.seed(42)
db <- dbscan(coords, eps = 0.05, minPts = 30)
spatial_data$Cluster <- as.factor(db$cluster)

cluster_summary <- spatial_data %>%
  filter(Cluster != "0") %>%
  group_by(Cluster) %>%
  summarise(
    DeliveryCount = n(),
    Lon           = mean(Longitude),
    Lat           = mean(Latitude),
    .groups       = "drop"
  ) %>%
  arrange(desc(DeliveryCount)) %>%
  slice(1:20) %>%
  mutate(ClusterRank = row_number())
```

## Join ClusterRank back onto spatial_data
```{r}
spatial_data <- spatial_data %>%
  left_join(cluster_summary %>% select(Cluster, ClusterRank),
            by = "Cluster") %>%
  filter(!is.na(ClusterRank))
```

## Resequencing Simulation & Visualization
```{r}
# Pick top-3 clusters by DeliveryCount
target_clusters <- cluster_summary$Cluster[1:3]
filtered_data   <- spatial_data %>% filter(Cluster %in% target_clusters)

# Define resequencing functions
resequence_greedy <- function(df) {
  D <- distm(df[, c("Longitude","Latitude")], fun = distHaversine)
  n <- nrow(df)
  path <- integer(n); visited <- rep(FALSE, n)
  path[1] <- 1; visited[1] <- TRUE
  for(i in 2:n) {
    last <- path[i-1]
    costs <- D[last, ]; costs[visited] <- Inf
    path[i] <- which.min(costs)
    visited[path[i]] <- TRUE
  }
  df[path, ]
}

resequence_weighted <- function(df) {
  D <- distm(df[, c("Longitude","Latitude")], fun = distHaversine)
  w <- df$PredictedTime
  n <- nrow(df)
  path <- integer(n); visited <- rep(FALSE, n)
  path[1] <- 1; visited[1] <- TRUE
  for(i in 2:n) {
    last <- path[i-1]
    costs <- D[last, ] * w; costs[visited] <- Inf
    path[i] <- which.min(costs)
    visited[path[i]] <- TRUE
  }
  df[path, ]
}

# Summarize distance improvements
summary_results <- lapply(target_clusters, function(cl) {
  td <- filtered_data %>% filter(Cluster == cl)
  if(nrow(td) < 4) return(NULL)
  if(!"LoadUtilization" %in% names(td)) {
    set.seed(123); td$LoadUtilization <- runif(nrow(td), 0.3, 0.9)
  }
  td <- td %>%
    filter(between(LoadUtilization, 0.3, 0.9)) %>%
    mutate(
      PackagesAtStop = sample(1:10, n(), TRUE),
      PredictedTime  = 9.3 - 1.17*LoadUtilization + 0.005*PackagesAtStop
    )
  orig <- td[, c("Longitude","Latitude")]
  grd  <- resequence_greedy(td)[, c("Longitude","Latitude")]
  wtd  <- resequence_weighted(td)[, c("Longitude","Latitude")]
  
  d0 <- sum(distHaversine(orig[-nrow(orig), ], orig[-1, ]))
  d1 <- sum(distHaversine(grd[-nrow(grd), ], grd[-1, ]))
  d2 <- sum(distHaversine(wtd[-nrow(wtd), ], wtd[-1, ]))
  
  tibble(
    Cluster           = cl,
    Original_Dist     = round(d0, 2),
    Greedy_Dist       = round(d1, 2),
    Weighted_Dist     = round(d2, 2),
    Greedy_Improve    = round((d0 - d1) / d0 * 100, 2),
    Weighted_Improve  = round((d0 - d2) / d0 * 100, 2)
  )
})
final_summary <- bind_rows(summary_results)
knitr::kable(final_summary, caption = "Resequencing Impact Across Top 3 Clusters")

# Legible three-panel plot
plot_resequence_comparison <- function(df, cl, zoom = 12, buffer = 0.05) {
  sub <- df %>% filter(Cluster == cl)
  if(nrow(sub) < 2) stop("Cluster too small: ", cl)
  if(!"LoadUtilization" %in% names(sub)) {
    set.seed(123); sub$LoadUtilization <- runif(nrow(sub), 0.3, 0.9)
  }
  sub <- sub %>%
    filter(between(LoadUtilization, 0.3, 0.9)) %>%
    mutate(
      PackagesAtStop = sample(1:10, n(), TRUE),
      PredictedTime  = 9.3 - 1.17*LoadUtilization + 0.005*PackagesAtStop,
      idx            = row_number()
    )
  bb <- make_bbox(lon = sub$Longitude, lat = sub$Latitude, f = buffer)
  mb <- get_map(location = bb, source = "google", maptype = "roadmap", zoom = zoom)
  
  orig <- sub
  grd  <- resequence_greedy(sub)   %>% mutate(idx = row_number())
  wtd  <- resequence_weighted(sub) %>% mutate(idx = row_number())
  endpoints <- function(d) d %>% filter(idx %in% c(1, max(idx)))
  
  # Original skeleton
  p0 <- ggmap(mb) +
    geom_path(data = orig, aes(Longitude, Latitude, group = 1),
              color = "black", size = 0.5, alpha = 0.5) +
    geom_point(data = orig, aes(Longitude, Latitude),
               color = "black", size = 1.5) +
    geom_label_repel(data = endpoints(orig),
                     aes(Longitude, Latitude, label = idx),
                     size = 3, box.padding = 0.2) +
    labs(title = paste("Cluster", cl, "– Original")) +
    theme_minimal()
  
  # Greedy: skeleton + greedy path + gradient stops
  p1 <- ggmap(mb) +
    geom_path(data = orig, aes(Longitude, Latitude, group = 1),
              color = "gray80", size = 0.3, alpha = 0.3) +
    geom_path(data = grd, aes(Longitude, Latitude, group = 1),
              color = "blue", size = 0.5, alpha = 0.7) +
    geom_point(data = grd, aes(Longitude, Latitude, color = idx),
               size = 2) +
    scale_color_viridis_c(name = "Stop\nOrder", option = "plasma") +
    geom_label_repel(data = endpoints(grd),
                     aes(Longitude, Latitude, label = idx),
                     size = 3, box.padding = 0.2) +
    labs(title = paste("Cluster", cl, "– Greedy")) +
    theme_minimal()
  
  # Weighted: skeleton + weighted path + gradient stops
  p2 <- ggmap(mb) +
    geom_path(data = orig, aes(Longitude, Latitude, group = 1),
              color = "gray80", size = 0.3, alpha = 0.3) +
    geom_path(data = wtd, aes(Longitude, Latitude, group = 1),
              color = "red", size = 0.5, alpha = 0.7) +
    geom_point(data = wtd, aes(Longitude, Latitude, color = idx),
               size = 2) +
    scale_color_viridis_c(name = "Stop\nOrder", option = "plasma") +
    geom_label_repel(data = endpoints(wtd),
                     aes(Longitude, Latitude, label = idx),
                     size = 3, box.padding = 0.2) +
    labs(title = paste("Cluster", cl, "– Weighted")) +
    theme_minimal()
  
  p0 | p1 | p2
}

# Render for each cluster
for(cl in target_clusters) {
  print(plot_resequence_comparison(filtered_data, cl))
}

knitr::kable(final_summary, caption = "Resequencing Impact Across Top Clusters")
```

# Resequencing (Correct Version)
## Load & Prepare Data
```{r}
spatial_data <- read.csv("/Users/Louie/Desktop/spatial_clustered_nationally.csv") %>%
  drop_na(Latitude, Longitude, LoadUtilization, DeliveryPeriod)

spatial_data <- spatial_data %>%
  mutate(DeliveryPeriod = str_extract(DeliveryPeriod, "Morning|Afternoon|Evening")) %>%
  mutate(TimePenalty = case_when(
    DeliveryPeriod == "Morning" ~ 1.5,
    DeliveryPeriod == "Afternoon" ~ 1.2,
    DeliveryPeriod == "Evening" ~ 1.0,
    TRUE ~ 1.0
  ))
```

## Define Weighted TSP Optimization Function
```{r}
optimize_weighted_tsp <- function(df) {
  coords <- df %>% select(Longitude, Latitude)
  dist_matrix <- distm(coords)
  weights <- df$TimePenalty / df$LoadUtilization
  weight_matrix <- outer(weights, weights, FUN = function(a, b) (a + b) / 2)

  if (!all(dim(dist_matrix) == dim(weight_matrix))) {
    stop("Weight matrix and distance matrix dimensions do not match.")
  }
# Apply the weights to the distance matrix
  dist_matrix <- dist_matrix * weight_matrix
  tsp <- TSP(as.dist(dist_matrix))
  route <- solve_TSP(tsp)
  df$OptimizedOrder <- match(seq_along(route), as.integer(route))
  return(df)
}
```

## Compute Total Route Distance Helper
```{r}
compute_total_distance <- function(df) {
  df <- df %>% arrange(OptimizedOrder)
  coords <- df %>% select(Longitude, Latitude)
  sum(distHaversine(coords[-nrow(coords), ], coords[-1, ]))
}
```

## Plotting Function for Cluster Routes
```{r}
# Function to plot original, TSP, and weighted-TSP routes for a given cluster
plot_cluster_routes <- function(cluster_id, max_n = 100) {
  cluster_df <- spatial_data %>%
  filter(Cluster == cluster_id,
         !is.na(TimePenalty),
         !is.na(LoadUtilization),
         LoadUtilization > 0) %>%
  slice_sample(n = max_n)


  coords <- cluster_df %>% select(Longitude, Latitude)
  dist_matrix <- distm(coords)

  tsp <- TSP(as.dist(dist_matrix))
  tsp_route <- solve_TSP(tsp)
  tsp_df <- cluster_df %>%
    mutate(OptimizedOrder = match(seq_along(tsp_route), as.integer(tsp_route)),
           Method = "TSP")

  weighted_df <- optimize_weighted_tsp(cluster_df) %>%
    mutate(Method = "Weighted TSP")

  base_df <- cluster_df %>%
    mutate(OptimizedOrder = row_number(),
           Method = "Original")

  combined <- bind_rows(base_df, tsp_df, weighted_df)

  plots <- list()
  distances <- list()
# Loop over each method to compute distances and generate maps
  for (method in c("Original", "TSP", "Weighted TSP")) {
    subset <- combined %>% filter(Method == method)
    dist <- compute_total_distance(subset)
    distances[[method]] <- dist

    p <- tryCatch({
      qmplot(Longitude, Latitude,
             data = subset %>% arrange(OptimizedOrder),
             maptype = "roadmap", zoom = 11, colour = OptimizedOrder,
             source = "google") +
        geom_path(aes(group = 1),
                  color = ifelse(method == "TSP", "red",
                                 ifelse(method == "Weighted TSP", "green", "blue")),
                  size = 2.0, alpha = 0.9) +
        geom_point(data = subset %>% filter(OptimizedOrder == 1),
                   aes(x = Longitude, y = Latitude),
                   color = "black", shape = 17, size = 4) +
        geom_point(data = subset %>% filter(OptimizedOrder == max(OptimizedOrder)),
                   aes(x = Longitude, y = Latitude),
                   color = "black", shape = 8, size = 4) +
        ggtitle(paste("Cluster", cluster_id, "-", method,
                      sprintf("(%.1f km)", dist / 1000)))
    }, error = function(e) {
      message("Map rendering failed for method ", method, ": ", e$message)
      ggplot() + ggtitle(paste("Error in", method))
    })

    plots[[method]] <- p
  }
# Arrange and display the three method plots vertically
  grid.arrange(grobs = plots, ncol = 1)
}

```

## Optimization Results for Cluster 1
```{r plot-cluster1, fig.height=12}
plot_cluster_routes(1)
```

## Optimization Results for Cluster 17
```{r plot-cluster17, fig.height=12}
plot_cluster_routes(17)
```
